{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94db0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../data/linkedin_posts.csv\")\n",
    "data2 = pd.read_csv(\"../data/hate_speech.csv\")\n",
    "data3 = pd.read_csv(\"../data/postings.csv\")\n",
    "\n",
    "length1 = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b36f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:716]\n",
    "\n",
    "#Append data2['text'] to data['Post Content']\n",
    "job_posting = pd.DataFrame({'Post Content': data3['description'].tolist()}).sample(frac=1, random_state=42).reset_index(drop=True).iloc[:500]\n",
    "data = pd.concat([data, job_posting], ignore_index=True)\n",
    "\n",
    "# Append data2['text'] to data['Post Content']\n",
    "hate_speech = pd.DataFrame({'Post Content': data2['text'].tolist()}).sample(frac=1, random_state=42).reset_index(drop=True).iloc[:500]\n",
    "data = pd.concat([data, hate_speech], ignore_index=True)\n",
    "\n",
    "# Convert all post contents to lowercase\n",
    "data['Post Content'] = [text.lower() for text in data['Post Content']]\n",
    "# Remove URLs\n",
    "data['Post Content'] = data['Post Content'].str.replace(r'\\bhttps?://\\S+\\b', '', regex=True)\n",
    "# Remove punctuation (periods, commas, question marks, double quotes, etc.)\n",
    "data['Post Content'] = data['Post Content'].str.replace(r'[^a-zA-Z ]', '', regex=True)\n",
    "\n",
    "post_content = data['Post Content'].tolist()\n",
    "job_des = data3['description'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848c346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Save post_content to a file called posts.txt\n",
    "with open(\"posts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    i = 0\n",
    "    for post in post_content:\n",
    "        i += 1\n",
    "        f.write(str(i) + \". \" + post + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7122ad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716\n",
      "1716\n"
     ]
    }
   ],
   "source": [
    "# Read classified.txt and append its content as a new column to the data dataframe\n",
    "with open(\"classified.txt\", \"r\", encoding=\"utf-8\") as f_classified:\n",
    "    classified_labels = [line.strip() for line in f_classified]\n",
    "    \n",
    "length2 = len(classified_labels)\n",
    "\n",
    "for i in range(length2):\n",
    "    if re.search(r'\\bnot\\b', str(classified_labels[i]), re.IGNORECASE):\n",
    "        classified_labels[i] = 0 #not cringe\n",
    "    else:\n",
    "        classified_labels[i] = 1 #cringe\n",
    "\n",
    "print(len(classified_labels)) #should be 716\n",
    "    \n",
    "classified_labels.extend([0] * len(job_posting)) #for the job postings\n",
    "    \n",
    "classified_labels.extend([1] * len(hate_speech)) #for the hate speech posts\n",
    "\n",
    "print(len(classified_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b5526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data\n",
    "\n",
    "final_data['Classified'] = classified_labels\n",
    "\n",
    "final_data = final_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_data = final_data.iloc[len(final_data)//2:].copy()\n",
    "final_data = final_data.iloc[:len(final_data)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa7b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=final_data[\"Post Content\"], vector_size=100, window=5, min_count=2, workers=4, sg = 1) #sg = 1 for skip gram\n",
    "word2vec_model.save(\"word2vec.model\") #save to file named word2vec.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf232c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 03:44:00.196408: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-30 03:44:00.199604: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-30 03:44:00.208532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751255040.218648   14511 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751255040.221702   14511 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751255040.231333   14511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751255040.231342   14511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751255040.231344   14511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751255040.231346   14511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-30 03:44:00.234626: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-30 03:44:03.324966: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9344</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">598,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m64,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9344\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m598,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">662,273</span> (2.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m662,273\u001b[0m (2.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">662,273</span> (2.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m662,273\u001b[0m (2.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "models = tf.keras.models\n",
    "layers = tf.keras.layers\n",
    "tokenizer_module = tf.keras.preprocessing.text\n",
    "sequence_module = tf.keras.preprocessing.sequence\n",
    "Tokenizer = tokenizer_module.Tokenizer\n",
    "pad_sequences = sequence_module.pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Model Hyperparameters (YOU NEED TO SET MAX_SEQUENCE_LENGTH) ---\n",
    "# This is the maximum number of words/tokens in any input post.\n",
    "MAX_SEQUENCE_LENGTH = 150 # Example: assuming most posts are <= 150 words\n",
    "EMBEDDING_DIM = 100       # This comes from your Word2Vec model's vector_size\n",
    "\n",
    "# Number of classes for binary classification (Good/Bad)\n",
    "NUM_CLASSES = 1 # For binary classification, output a single probability\n",
    "\n",
    "# --- 2. Build the CNN Model ---\n",
    "def build_text_cnn_model(max_seq_length, embedding_dim, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Layer 1: Conv1D (Convolutional Layer for text)\n",
    "    # filters: Number of feature detectors/filters to learn (e.g., to detect n-gram patterns)\n",
    "    # kernel_size: The size of the sliding window (e.g., 5 words at a time)\n",
    "    # activation: ReLU for non-linearity\n",
    "    # input_shape: (sequence_length, embedding_dimension)\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu',\n",
    "                            input_shape=(max_seq_length, embedding_dim)))\n",
    "\n",
    "    # Layer 2: MaxPooling1D (Pooling Layer for text)\n",
    "    # pool_size: The size of the pooling window. Reduces sequence length.\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    # This will reduce the length of the sequence by half.\n",
    "\n",
    "    # Optional:\n",
    "    # model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    # model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Layer 3: Flatten Layer\n",
    "    # Converts the 2D output from MaxPooling1D (length x filters) into a 1D vector\n",
    "    # This prepares the data for the fully connected (Dense) layers.\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Layer 4: Dense (Fully Connected) Hidden Layer\n",
    "    # units: Number of neurons in this layer.\n",
    "    # activation: ReLU for non-linearity.\n",
    "    model.add(layers.Dense(units=64, activation='relu'))\n",
    "    # You can experiment with the number of units here.\n",
    "\n",
    "    # Layer 5: Dense (Output Layer)\n",
    "    # units: 1 for binary classification (predicting a probability).\n",
    "    # activation: 'sigmoid' to output a probability between 0 and 1.\n",
    "    model.add(layers.Dense(units=num_classes, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model instance\n",
    "model = build_text_cnn_model(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, NUM_CLASSES)\n",
    "\n",
    "# Print a summary of the model's architecture\n",
    "model.summary()\n",
    "\n",
    "# --- 3. Compile the Model ---\n",
    "# optimizer: How the model updates its weights during training. Adam is a good default.\n",
    "# loss: Binary Crossentropy for binary classification problems.\n",
    "# metrics: What to monitor during training (e.g., accuracy).\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2372c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 03:44:04.049021: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 41160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5304 - loss: 0.6692 - val_accuracy: 0.6919 - val_loss: 0.5601\n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7051 - loss: 0.5732 - val_accuracy: 0.7558 - val_loss: 0.5789\n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7691 - loss: 0.5037 - val_accuracy: 0.7442 - val_loss: 0.5814\n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8006 - loss: 0.4316 - val_accuracy: 0.7558 - val_loss: 0.5753\n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8085 - loss: 0.3794 - val_accuracy: 0.7791 - val_loss: 0.5895\n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8370 - loss: 0.3615 - val_accuracy: 0.8023 - val_loss: 0.6385\n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8606 - loss: 0.3195 - val_accuracy: 0.7733 - val_loss: 0.6698\n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8544 - loss: 0.2847 - val_accuracy: 0.8023 - val_loss: 0.6998\n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8724 - loss: 0.2798 - val_accuracy: 0.7965 - val_loss: 0.7508\n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8825 - loss: 0.2483 - val_accuracy: 0.8081 - val_loss: 0.7580\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8841 - loss: 0.2638\n",
      "Test Accuracy: 88.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 03:44:08.967694: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 41160000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Prepare Your Data (Conceptual Steps - you'll need to implement this) ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Tokenize your posts (convert text to lists of words)\n",
    "# Example (using Keras's Tokenizer for simplicity, but you might use custom tokenization):\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(final_data['Post Content'].tolist()) # Fit tokenizer on your post content\n",
    "word_index = tokenizer.word_index # Vocabulary mapping word to ID\n",
    "\n",
    "# 2. Convert posts to sequences of word IDs (using your tokenizer)\n",
    "sequences = tokenizer.texts_to_sequences(final_data['Post Content'].tolist())\n",
    "\n",
    "# 3. Pad/Truncate sequences to MAX_SEQUENCE_LENGTH\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# 4. Convert padded sequences of word IDs into sequences of Word2Vec vectors\n",
    "# This is the crucial step to get the input shape (None, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "X = np.zeros((len(padded_sequences), MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "for i, sequence in enumerate(padded_sequences):\n",
    "    for j, word_id in enumerate(sequence):\n",
    "        word = tokenizer.index_word.get(word_id) # Convert ID back to word\n",
    "        if word in word2vec_model.wv: # Check if word is in Word2Vec vocabulary\n",
    "            X[i, j] = word2vec_model.wv[word]\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words (e.g., keep as zeros or use a special UNK vector)\n",
    "            pass\n",
    "y = np.array(final_data['Classified'].tolist()) # Convert labels to numpy array\n",
    "\n",
    "# --- 5. Split Data (Training, Validation) ---\n",
    "# This code splits your data into training and validation sets, but does not create a separate test set.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 6. Train the Model (Conceptual) ---\n",
    "model.fit(X_train, y_train,\n",
    "            epochs=10, # Number of training iterations\n",
    "            batch_size=32, # Number of samples per gradient update\n",
    "            validation_data=(X_val, y_val)) # Data to evaluate on after each epoch\n",
    "\n",
    "# --- 7. Evaluate the Model (Conceptual) ---\n",
    "loss, accuracy = model.evaluate(X_train, y_train) ##fix to x and y test later!!\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4578e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 03:45:47.770026: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 51480000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess test_data[\"Post Content\"] to match model input\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data[\"Post Content\"].tolist())\n",
    "test_padded = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "X_test = np.zeros((len(test_padded), MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "for i, sequence in enumerate(test_padded):\n",
    "\tfor j, word_id in enumerate(sequence):\n",
    "\t\tword = tokenizer.index_word.get(word_id)\n",
    "\t\tif word in word2vec_model.wv:\n",
    "\t\t\tX_test[i, j] = word2vec_model.wv[word]\n",
    "\n",
    "y_test = np.array(test_data[\"Classified\"].tolist())\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e84aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one month into my ms marketing program a researchintensive degree at iba i found myself turning to chatgpt for validation of my english language skills everyweek i am assigned multiple readings in all four of my courses to complete before the next session and while i am not someone who enjoys reading particularly i have been putting in consistent effort to do what is expected of me academic readings however are a completely different ballgame i often find myself reading a sentence multiple times yet still struggling to understand its meaning it feels as if english has betrayed methough the words seem familiar the sentence as a whole doesnt make sense ai tools like chatgpt and copilot ai have been incredibly helpful in clarifying difficult passages my current approach is to first read a simplified aigenerated summary of an article or paper which helps me grasp the general idea before diving into the full text if anyone has helpful tips for an art student looking to improve her reading skills and better tackle academic papers please share more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "This post is classified as GOOD.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 8. Make Predictions (Conceptual) ---\n",
    "new_post_text = \"\"\n",
    "#Try taking a random post from the test_data that is classified as \"not bad\" (0)\n",
    "for i in range(len(test_data)):\n",
    "    if test_data['Classified'].iloc[i] == 0:\n",
    "        new_post_text = test_data['Post Content'].iloc[i]\n",
    "        break\n",
    "    \n",
    "print(new_post_text)\n",
    "\n",
    "# Preprocess new_post_text into a single Word2Vec sequence (X_new)\n",
    "sequence = tokenizer.texts_to_sequences([new_post_text])\n",
    "padded = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "X_new = np.zeros((1, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "for j, word_id in enumerate(padded[0]):\n",
    "    word = tokenizer.index_word.get(word_id)\n",
    "    if word in word2vec_model.wv:\n",
    "        X_new[0, j] = word2vec_model.wv[word]\n",
    "\n",
    "prediction_proba = model.predict(X_new)[0][0]\n",
    "if prediction_proba > 0.5: # Threshold for \"bad\"\n",
    "    print(\"This post is classified as BAD.\")\n",
    "else:\n",
    "    print(\"This post is classified as GOOD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57242cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop saying that refugees are vile\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "This post is classified as BAD.\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Make Predictions (Conceptual) ---\n",
    "new_post_text = \"\"\n",
    "#Try taking a random post from the test_data that is classified as \"bad\" (1)\n",
    "for i in range(len(test_data)):\n",
    "    if test_data['Classified'].iloc[i] == 1:\n",
    "        new_post_text = test_data['Post Content'].iloc[i]\n",
    "        break\n",
    "    \n",
    "print(new_post_text)\n",
    "\n",
    "# Preprocess new_post_text into a single Word2Vec sequence (X_new)\n",
    "sequence = tokenizer.texts_to_sequences([new_post_text])\n",
    "padded = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "X_new = np.zeros((1, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "for j, word_id in enumerate(padded[0]):\n",
    "    word = tokenizer.index_word.get(word_id)\n",
    "    if word in word2vec_model.wv:\n",
    "        X_new[0, j] = word2vec_model.wv[word]\n",
    "\n",
    "prediction_proba = model.predict(X_new)[0][0]\n",
    "if prediction_proba > 0.5: # Threshold for \"bad\"\n",
    "    print(\"This post is classified as BAD.\")\n",
    "else:\n",
    "    print(\"This post is classified as GOOD.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
