{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94db0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../data/linkedin_posts.csv\")\n",
    "data2 = pd.read_csv(\"../data/hate_speech.csv\")\n",
    "\n",
    "length1 = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6b36f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append data2['text'] to data['Post Content']\n",
    "new_rows = pd.DataFrame({'Post Content': data2['text'].tolist()})\n",
    "data = pd.concat([data, new_rows], ignore_index=True)\n",
    "# Convert all post contents to lowercase\n",
    "data['Post Content'] = [text.lower() for text in data['Post Content']]\n",
    "# Remove URLs\n",
    "data['Post Content'] = data['Post Content'].str.replace(r'\\bhttps?://\\S+\\b', '', regex=True)\n",
    "# Remove punctuation (periods, commas, question marks, double quotes, etc.)\n",
    "data['Post Content'] = data['Post Content'].str.replace(r'[^a-zA-Z ]', '', regex=True)\n",
    "\n",
    "post_content = data['Post Content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "848c346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Save post_content to a file called posts.txt\n",
    "with open(\"posts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    i = 0\n",
    "    for post in post_content:\n",
    "        i += 1\n",
    "        f.write(str(i) + \". \" + post + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7122ad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716\n",
      "41339\n"
     ]
    }
   ],
   "source": [
    "# Read classified.txt and append its content as a new column to the data dataframe\n",
    "with open(\"classified.txt\", \"r\", encoding=\"utf-8\") as f_classified:\n",
    "    classified_labels = [line.strip() for line in f_classified]\n",
    "    \n",
    "length2 = len(classified_labels)\n",
    "\n",
    "for i in range(length2):\n",
    "    if re.search(r'\\bnot\\b', str(classified_labels[i]), re.IGNORECASE):\n",
    "        classified_labels[i] = 0 #not cringe\n",
    "    else:\n",
    "        classified_labels[i] = 1 #cringe\n",
    "\n",
    "print(len(classified_labels)) #should be 716\n",
    "    \n",
    "classified_labels.extend([1] * len(data2)) #for the hate speech posts\n",
    "\n",
    "print(len(classified_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4b5526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data.drop(index=range(length2, length1)).reset_index(drop=True)\n",
    "final_data['Classified'] = classified_labels\n",
    "\n",
    "final_data = final_data.iloc[:2400]\n",
    "\n",
    "final_data = final_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_data = final_data.iloc[1200:].copy()\n",
    "final_data = final_data.iloc[:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cfa7b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=final_data[\"Post Content\"], vector_size=100, window=5, min_count=2, workers=4, sg = 1) #sg = 1 for skip gram\n",
    "word2vec_model.save(\"word2vec.model\") #save to file named word2vec.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6a95713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_all_caps_words': 0, 'num_exclamation_marks': 1, 'num_question_marks': 0, 'is_selling_course_keyword_present': True, 'is_emotional_story_keyword_present': False, 'is_clickbait_headline_present': True, 'is_tag_people_call_to_action': False, 'is_comment_interested_cta': False, 'is_humble_brag_keyword_present': False, 'post_length_words': 18, 'post_length_chars': 136, 'is_purely_personal_topic': False, 'is_generic_advice': False, 'exclamation_to_word_ratio': 0.05555555555555555}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def analyze_linkedin_post(paragraph: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyzes a LinkedIn post paragraph for emotional baiting keywords,\n",
    "    structural features, and semantic indicators.\n",
    "\n",
    "    Args:\n",
    "        paragraph (str): The text content of the LinkedIn post.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing various analysis features.\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # Count words that are all caps (and longer than 1 character to exclude 'I' or 'A')\n",
    "    all_caps_words_count = 0\n",
    "    for word in re.findall(r'\\b[A-Z]+\\b', paragraph): # Find all uppercase words in original case\n",
    "        if len(word) > 1: # Exclude single-letter words like \"I\" or \"A\"\n",
    "            all_caps_words_count += 1\n",
    "    results['num_all_caps_words'] = all_caps_words_count\n",
    "    \n",
    "    results['num_exclamation_marks'] = paragraph.count('!')\n",
    "    results['num_question_marks'] = paragraph.count('?')\n",
    "\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    lower_paragraph = paragraph.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', lower_paragraph) # Tokenize words, stripping punctuation\n",
    "    num_words = len(words)\n",
    "    num_chars = len(paragraph)\n",
    "\n",
    "    # --- Keyword/Phrase Definitions (Case-insensitive matching) ---\n",
    "    selling_course_keywords = [\"buy\", \"enroll now\", \"masterclass\", \"exclusive program\",\n",
    "                                \"webinar\", \"bootcamp\", \"course\", \"training\", \"workshop\",\n",
    "                                \"learn how to\", \"join now\", \"sign up\"]\n",
    "\n",
    "    emotional_story_keywords = [\"my journey\", \"struggle\", \"struggled\", \"tears\",\n",
    "                                \"breakthrough\", \"against all odds\", \"hit rock bottom\",\n",
    "                                \"lowest point\", \"lost\", \"sacrifice\", \"sacrificed\",\n",
    "                                \"overwhelmed\", \"defeated\", \"almost gave up\", \"resilience\",\n",
    "                                \"perseverance\", \"vulnerability\"]\n",
    "\n",
    "    clickbait_headline_regex = re.compile(\n",
    "        r\"(you won't believe|the secret to|this one trick|shocking truth|mind-blowing|\"\n",
    "        r\"game-changer|defied expectations|achieved the impossible|unprecedented|\"\n",
    "        r\"revolutionary|what happened next|revealed|hidden truth)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    tag_people_cta_keywords = [\"tag\", \"like if you agree\", \"share this post\",\n",
    "                                \"comment if you agree\", \"react with\", \"mention a friend\",\n",
    "                                \"spread the word\", \"repost\"]\n",
    "\n",
    "    comment_interested_cta_keywords = [\"comment 'interested'\", \"type 'yes'\",\n",
    "                                        \"dm me\", \"message me\", \"inbox me\",\n",
    "                                        \"comment below\", \"let me know\",\n",
    "                                        \"drop a comment\", \"tell me your thoughts\",\n",
    "                                        \"what do you think\", \"join the conversation\",\n",
    "                                        \"let's discuss\", \"share your opinion\",\n",
    "                                        \"i want to hear from you\", \"let's connect\",\n",
    "                                        \"i'd love to know\", \"what's your take\",\n",
    "                                        \"i'm curious\", \"let's chat\", \"let's talk\",\n",
    "                                        \"i'm interested\", \"let's engage\", \"let's collaborate\",\n",
    "                                        \"let's brainstorm\", \"i want to hear\"]\n",
    "\n",
    "    humble_brag_keywords = [\"blessed\", \"didn't expect this\",\n",
    "                            \"just reached\", \"grateful\", \"honored\", \"overwhelmed\",\n",
    "                            \"speechless\", \"never imagined\", \"a dream come true\", \"pinch myself\"]\n",
    "\n",
    "    purely_personal_topic_keywords = [\"vacation\", \"family\", \"kids\", \"children\",\n",
    "                                        \"wedding\", \"anniversary\", \"birthday\", \"pets\",\n",
    "                                        \"dog\", \"cat\", \"home life\", \"weekend vibes\", \"my life\",\n",
    "                                        \"personal story\", \"holiday\", \"my spouse\", \"partner\",\n",
    "                                        \"date night\", \"travel\", \"adventure\", \"my journey\",\n",
    "                                        \"self-care\", \"mental health\", \"wellness\",\n",
    "                                        \"hobbies\", \"interests\", \"lifestyle\", \"daily routine\",\n",
    "                                        \"life update\", \"my passion\", \"hobby\"]\n",
    "\n",
    "    generic_advice_keywords = [\"never give up\", \"keep learning\", \"be persistent\",\n",
    "                                \"stay hungry\", \"stay foolish\", \"growth mindset\",\n",
    "                                \"daily habits\", \"consistency is key\", \"believe in yourself\",\n",
    "                                \"manifest your dreams\", \"your why\", \"find your passion\",\n",
    "                                \"embrace failure\", \"learn from mistakes\", \"stay positive\"]\n",
    "\n",
    "    # --- Presence of Keywords/Phrases ---\n",
    "    def check_keywords_presence(text, keywords):\n",
    "        return any(keyword in text for keyword in keywords)\n",
    "\n",
    "    results['is_selling_course_keyword_present'] = check_keywords_presence(lower_paragraph, selling_course_keywords)\n",
    "    results['is_emotional_story_keyword_present'] = check_keywords_presence(lower_paragraph, emotional_story_keywords)\n",
    "    results['is_clickbait_headline_present'] = bool(clickbait_headline_regex.search(paragraph)) # Use original case for regex\n",
    "    results['is_tag_people_call_to_action'] = check_keywords_presence(lower_paragraph, tag_people_cta_keywords)\n",
    "    results['is_comment_interested_cta'] = check_keywords_presence(lower_paragraph, comment_interested_cta_keywords)\n",
    "    results['is_humble_brag_keyword_present'] = check_keywords_presence(lower_paragraph, humble_brag_keywords)\n",
    "    results['post_length_words'] = num_words\n",
    "    results['post_length_chars'] = num_chars\n",
    "\n",
    "\n",
    "    # --- Semantic/Topical Features ---\n",
    "    results['is_purely_personal_topic'] = check_keywords_presence(lower_paragraph, purely_personal_topic_keywords)\n",
    "    results['is_generic_advice'] = check_keywords_presence(lower_paragraph, generic_advice_keywords)\n",
    "\n",
    "    # --- Ratio-based Features ---\n",
    "    results['exclamation_to_word_ratio'] = results['num_exclamation_marks'] / num_words if num_words > 0 else 0\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "example_paragraph = \"I just completed a masterclass on emotional intelligence! You won't believe the insights I gained. #EmotionalIntelligence #GrowthMindset\"\n",
    "analysis_results = analyze_linkedin_post(example_paragraph)\n",
    "print(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4bf232c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9344</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">598,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m64,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9344\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m598,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">662,273</span> (2.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m662,273\u001b[0m (2.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">662,273</span> (2.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m662,273\u001b[0m (2.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "models = tf.keras.models\n",
    "layers = tf.keras.layers\n",
    "tokenizer_module = tf.keras.preprocessing.text\n",
    "sequence_module = tf.keras.preprocessing.sequence\n",
    "Tokenizer = tokenizer_module.Tokenizer\n",
    "pad_sequences = sequence_module.pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Model Hyperparameters (YOU NEED TO SET MAX_SEQUENCE_LENGTH) ---\n",
    "# This is the maximum number of words/tokens in any input post.\n",
    "# You need to determine this based on your dataset's average/max post length.\n",
    "# A good starting point is often the 90th or 95th percentile of your post lengths.\n",
    "MAX_SEQUENCE_LENGTH = 150 # Example: assuming most posts are <= 150 words\n",
    "EMBEDDING_DIM = 100       # This comes from your Word2Vec model's vector_size\n",
    "\n",
    "# Number of classes for binary classification (Good/Bad)\n",
    "NUM_CLASSES = 1 # For binary classification, output a single probability\n",
    "\n",
    "# --- 2. Build the CNN Model ---\n",
    "def build_text_cnn_model(max_seq_length, embedding_dim, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Layer 1: Conv1D (Convolutional Layer for text)\n",
    "    # filters: Number of feature detectors/filters to learn (e.g., to detect n-gram patterns)\n",
    "    # kernel_size: The size of the sliding window (e.g., 5 words at a time)\n",
    "    # activation: ReLU for non-linearity\n",
    "    # input_shape: (sequence_length, embedding_dimension)\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu',\n",
    "                            input_shape=(max_seq_length, embedding_dim)))\n",
    "    # Note: We're starting with 128 filters. You can experiment with this.\n",
    "\n",
    "    # Layer 2: MaxPooling1D (Pooling Layer for text)\n",
    "    # pool_size: The size of the pooling window. Reduces sequence length.\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    # This will reduce the length of the sequence by half.\n",
    "\n",
    "    # Optional: You can add more Conv1D and MaxPooling1D layers for deeper feature extraction\n",
    "    # model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    # model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Layer 3: Flatten Layer\n",
    "    # Converts the 2D output from MaxPooling1D (length x filters) into a 1D vector\n",
    "    # This prepares the data for the fully connected (Dense) layers.\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Layer 4: Dense (Fully Connected) Hidden Layer\n",
    "    # units: Number of neurons in this layer.\n",
    "    # activation: ReLU for non-linearity.\n",
    "    model.add(layers.Dense(units=64, activation='relu'))\n",
    "    # You can experiment with the number of units here.\n",
    "\n",
    "    # Layer 5: Dense (Output Layer)\n",
    "    # units: 1 for binary classification (predicting a probability).\n",
    "    # activation: 'sigmoid' to output a probability between 0 and 1.\n",
    "    model.add(layers.Dense(units=num_classes, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model instance\n",
    "model = build_text_cnn_model(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, NUM_CLASSES)\n",
    "\n",
    "# Print a summary of the model's architecture\n",
    "model.summary()\n",
    "\n",
    "# --- 3. Compile the Model ---\n",
    "# optimizer: How the model updates its weights during training. Adam is a good default.\n",
    "# loss: Binary Crossentropy for binary classification problems.\n",
    "# metrics: What to monitor during training (e.g., accuracy).\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e2372c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7482 - loss: 0.5819 - val_accuracy: 0.8250 - val_loss: 0.3871\n",
      "Epoch 2/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8748 - loss: 0.3189 - val_accuracy: 0.8583 - val_loss: 0.3358\n",
      "Epoch 3/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9033 - loss: 0.2587 - val_accuracy: 0.8583 - val_loss: 0.3562\n",
      "Epoch 4/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9019 - loss: 0.2612 - val_accuracy: 0.8708 - val_loss: 0.3685\n",
      "Epoch 5/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9279 - loss: 0.2087 - val_accuracy: 0.8583 - val_loss: 0.3691\n",
      "Epoch 6/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9393 - loss: 0.1972 - val_accuracy: 0.8583 - val_loss: 0.4180\n",
      "Epoch 7/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9489 - loss: 0.1724 - val_accuracy: 0.8583 - val_loss: 0.4178\n",
      "Epoch 8/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9594 - loss: 0.1638 - val_accuracy: 0.8583 - val_loss: 0.4626\n",
      "Epoch 9/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9488 - loss: 0.1705 - val_accuracy: 0.8625 - val_loss: 0.4915\n",
      "Epoch 10/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9443 - loss: 0.1746 - val_accuracy: 0.8625 - val_loss: 0.5156\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9554 - loss: 0.1530\n",
      "Test Accuracy: 95.73%\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Prepare Your Data (Conceptual Steps - you'll need to implement this) ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have:\n",
    "# - `all_posts`: A list of your raw text strings (e.g., [\"Just published a new article...\", \"This course is great...\", ...])\n",
    "# - `all_labels`: A list of corresponding labels (e.g., [0, 1, 0, 1, ...], where 0=Good, 1=Bad)\n",
    "# - `word2vec_model`: Your trained Word2Vec model\n",
    "\n",
    "# 1. Tokenize your posts (convert text to lists of words)\n",
    "# Example (using Keras's Tokenizer for simplicity, but you might use custom tokenization):\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(final_data['Post Content'].tolist()) # Fit tokenizer on your post content\n",
    "word_index = tokenizer.word_index # Vocabulary mapping word to ID\n",
    "\n",
    "# 2. Convert posts to sequences of word IDs (using your tokenizer)\n",
    "sequences = tokenizer.texts_to_sequences(final_data['Post Content'].tolist())\n",
    "\n",
    "# 3. Pad/Truncate sequences to MAX_SEQUENCE_LENGTH\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# 4. Convert padded sequences of word IDs into sequences of Word2Vec vectors\n",
    "# This is the crucial step to get the input shape (None, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "# Example:\n",
    "X = np.zeros((len(padded_sequences), MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "for i, sequence in enumerate(padded_sequences):\n",
    "    for j, word_id in enumerate(sequence):\n",
    "        word = tokenizer.index_word.get(word_id) # Convert ID back to word\n",
    "        if word in word2vec_model.wv: # Check if word is in Word2Vec vocabulary\n",
    "            X[i, j] = word2vec_model.wv[word]\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words (e.g., keep as zeros or use a special UNK vector)\n",
    "            pass\n",
    "y = np.array(final_data['Classified'].tolist()) # Convert labels to numpy array\n",
    "\n",
    "# --- 5. Split Data (Training, Validation) ---\n",
    "# This code splits your data into training and validation sets, but does not create a separate test set.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 6. Train the Model (Conceptual) ---\n",
    "model.fit(X_train, y_train,\n",
    "            epochs=10, # Number of training iterations\n",
    "            batch_size=32, # Number of samples per gradient update\n",
    "            validation_data=(X_val, y_val)) # Data to evaluate on after each epoch\n",
    "\n",
    "# --- 7. Evaluate the Model (Conceptual) ---\n",
    "loss, accuracy = model.evaluate(X_train, y_train) ##fix to x and y test later!!\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b1e84aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automation vs human touch customer experience the ultimate balance real scenarios ive faced  customer service  ai chatbots or human representatives  manufacturing  robotic efficiency or artisanal quality  data analysis  algorithmdriven or humaninterpreted  creative work  aigenerated or humancrafted  decisionmaking  datadriven or experiencebased theres no onesizefitsall solution for the future workplace what ive learned  automation improves efficiency but human touch drives loyalty  the perfect mix hightech hightouch  people crave personalization even in a digital world  continuous learning is the new job security  the best results often come from humanai collaboration the takeaway the future isnt human vs machine its human and machine  repost  to help others hit the  to get notified follow nitin mathur   hashtag  automationvshuman hashtag  customerexperience hashtag  aiethics hashtag  businessinnovation hashtag  linkedinautomation more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "This post is classified as GOOD.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 8. Make Predictions (Conceptual) ---\n",
    "new_post_text = \"\"\n",
    "for i in range(len(test_data)):\n",
    "    if test_data['Classified'].iloc[i] == 0:\n",
    "        new_post_text = test_data['Post Content'].iloc[i]\n",
    "        break\n",
    "    \n",
    "print(new_post_text)\n",
    "\n",
    "# Preprocess new_post_text into a single Word2Vec sequence (X_new)\n",
    "sequence = tokenizer.texts_to_sequences([new_post_text])\n",
    "padded = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "X_new = np.zeros((1, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "for j, word_id in enumerate(padded[0]):\n",
    "    word = tokenizer.index_word.get(word_id)\n",
    "    if word in word2vec_model.wv:\n",
    "        X_new[0, j] = word2vec_model.wv[word]\n",
    "    # else: leave as zeros\n",
    "\n",
    "prediction_proba = model.predict(X_new)[0][0]\n",
    "if prediction_proba > 0.5: # Threshold for \"bad\"\n",
    "    print(\"This post is classified as BAD.\")\n",
    "else:\n",
    "    print(\"This post is classified as GOOD.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
